{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üé≠ Fine-tuning LLM –Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö –ê–ª–º–∞—Ç—ã —Å QLoRA + ORPO\n\n## –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞\n\n–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã –æ–±—É—á–∞–µ–º —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å **Qwen2.5-3B** –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –ê–ª–º–∞—Ç—ã (sxodim.com), –∏—Å–ø–æ–ª—å–∑—É—è:\n- **QLoRA** ‚Äî –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π\n- **ORPO** ‚Äî –¥–ª—è –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ, —á–µ–ª–æ–≤–µ—á–Ω–æ–≥–æ —Å—Ç–∏–ª—è –æ—Ç–≤–µ—Ç–æ–≤\n\n### –ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è:\n```\n–ë–∞–∑–∞ (Qwen2.5-3B) ‚Üí SFT (–∑–Ω–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤) ‚Üí ORPO (–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Å—Ç–∏–ª—å)\n```\n\n### –ß—Ç–æ —Ç–∞–∫–æ–µ QLoRA + ORPO?\n\n| –ú–µ—Ç–æ–¥ | –¶–µ–ª—å | –†–µ–∑—É–ª—å—Ç–∞—Ç |\n|-------|------|-----------|\n| **QLoRA** | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | ~6GB –ø–∞–º—è—Ç–∏ –≤–º–µ—Å—Ç–æ 48GB |\n| **SFT** | –ù–∞—É—á–∏—Ç—å —Ñ–∞–∫—Ç–∞–º | –ú–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö |\n| **ORPO** | –ù–∞—É—á–∏—Ç—å —Å—Ç–∏–ª—é | –û—Ç–≤–µ—á–∞–µ—Ç –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –∂–∏–≤–æ |\n\n### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ—É—Ç–±—É–∫–∞:\n1. **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**\n2. **–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π**\n3. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤**\n4. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞**\n5. **SFT –æ–±—É—á–µ–Ω–∏–µ** (—Ñ–∞–∫—Ç—ã –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö)\n6. **ORPO –æ–±—É—á–µ–Ω–∏–µ** (–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Å—Ç–∏–ª—å)\n7. **–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏**\n8. **–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**\n9. **–≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º **Unsloth** ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ fine-tuning LLM.\n",
    "\n",
    "Unsloth –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:\n",
    "- 2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "- 60% –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏\n",
    "- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ uv (–±—ã—Å—Ç—Ä—ã–π –ø–∞–∫–µ—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä)\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "# 1. –ë–∞–∑–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è deep learning\n",
    "!uv pip install -qqq \\\n",
    "  \"torch>=2.8.0\" \\\n",
    "  \"triton>=3.4.0\" \\\n",
    "  torchvision \\\n",
    "  bitsandbytes==0.48.0 \\\n",
    "  transformers==4.56.2\n",
    "\n",
    "# 2. Unsloth (–í–ê–ñ–ù–û: —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å GitHub, –Ω–µ –∏–∑ PyPI!)\n",
    "!uv pip install -qqq \\\n",
    "  \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "  \"unsloth[base] @ git+https://github.com/unslothai/unsloth\"\n",
    "\n",
    "# 3. TRL –¥–ª—è SFT –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)\n",
    "!uv pip install -qqq --no-deps trl==0.22.2\n",
    "\n",
    "# 4. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "!uv pip install -qqq evaluate bert_score sacrebleu rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (QLoRA)\n",
    "\n",
    "### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏: Qwen2.5-3B\n",
    "\n",
    "**Qwen2.5-3B** ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Alibaba:\n",
    "- 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "- –û—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤\n",
    "- –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 32K —Ç–æ–∫–µ–Ω–æ–≤\n",
    "- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è instruction-following\n",
    "\n",
    "### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏:\n",
    "- `load_in_4bit=True` ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –≤ 4-bit —Ñ–æ—Ä–º–∞—Ç–µ (NF4)\n",
    "- `bnb_4bit_compute_dtype=float16` ‚Äî –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ FP16\n",
    "- `bnb_4bit_quant_type=\"nf4\"` ‚Äî NormalFloat4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è\n",
    "- `bnb_4bit_use_double_quant=True` ‚Äî –¥–≤–æ–π–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—ç–∫–æ–Ω–æ–º–∏—Ç –µ—â—ë ~0.4 –±–∏—Ç/–ø–∞—Ä–∞–º–µ—Ç—Ä)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "MODEL_NAME = \"unsloth/Qwen2.5-3B-bnb-4bit\"  # –ü—Ä–µ–¥–∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è 4-bit –≤–µ—Ä—Å–∏—è\n",
    "MAX_SEQ_LENGTH = 2048                        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,              # QLoRA: 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è\n",
    "    dtype=None,                     # Auto-detect (float16 –¥–ª—è GPU)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {MODEL_NAME}\")\n",
    "print(f\"üìä –¢–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: 4-bit NF4 (QLoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "\n",
    "### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA:\n",
    "\n",
    "| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |\n",
    "|----------|----------|----------|\n",
    "| `r` | 32 | –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü A –∏ B. –í—ã—à–µ = –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ |\n",
    "| `lora_alpha` | 64 | –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –û–±—ã—á–Ω–æ `alpha = 2 * r` |\n",
    "| `lora_dropout` | 0.05 | Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ |\n",
    "| `target_modules` | [q, k, v, o, gate, up, down] | –°–ª–æ–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ |\n",
    "\n",
    "### –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏:\n",
    "- **Attention**: `q_proj`, `k_proj`, `v_proj`, `o_proj`\n",
    "- **MLP/FFN**: `gate_proj`, `up_proj`, `down_proj`\n",
    "\n",
    "–ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,                    # –†–∞–Ω–≥ LoRA (–≤—ã—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)\n",
    "    lora_alpha=64,           # Alpha = 2 * r (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)\n",
    "    lora_dropout=0.05,       # –ù–µ–±–æ–ª—å—à–æ–π dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP layers\n",
    "    ],\n",
    "    bias=\"none\",             # –ù–µ –æ–±—É—á–∞–µ–º bias (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "    use_gradient_checkpointing=\"unsloth\",  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ backprop\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "frozen = total - trainable\n",
    "\n",
    "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\")\n",
    "print(f\"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:    {total:>15,}\")\n",
    "print(f\"   –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö:        {frozen:>15,} ({100*frozen/total:.2f}%)\")\n",
    "print(f\"   –û–±—É—á–∞–µ–º—ã—Ö (QLoRA):   {trainable:>15,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"\\n‚úÖ QLoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\n–ó–∞–≥—Ä—É–∂–∞–µ–º SFT –¥–∞—Ç–∞—Å–µ—Ç –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –ê–ª–º–∞—Ç—ã (sxodim.com).\n\n### –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:\n```json\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –≤ –ê–ª–º–∞—Ç—ã...\"},\n    {\"role\": \"user\", \"content\": \"–ö—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö?\"},\n    {\"role\": \"assistant\", \"content\": \"–†–µ–∫–æ–º–µ–Ω–¥—É—é –ø–æ—Å–µ—Ç–∏—Ç—å –∫–æ–Ω—Ü–µ—Ä—Ç...\"}\n  ]\n}\n```\n\n### –§–æ—Ä–º–∞—Ç —á–∞—Ç–∞ Qwen2.5 (ChatML):\n```\n<|im_start|>system\n–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –≤ –ê–ª–º–∞—Ç—ã.<|im_end|>\n<|im_start|>user\n–í–æ–ø—Ä–æ—Å?<|im_end|>\n<|im_start|>assistant\n–û—Ç–≤–µ—Ç.<|im_end|>\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\nDATASET_PATH = \"/content/sft_dataset_chat.json\"  # –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –ê–ª–º–∞—Ç—ã\n\nwith open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n    sft_data = json.load(f)\n\nprint(f\"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(sft_data)}\")\n\n# –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä\nprint(\"\\nüìù –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\nexample = sft_data[50]\nfor msg in example[\"messages\"]:\n    role = msg[\"role\"].upper()\n    content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n    print(f\"   [{role}]: {content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ chat template –¥–ª—è Qwen2.5 (ChatML —Ñ–æ—Ä–º–∞—Ç)\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "        \"{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "        \"{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "        \"{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}\"\n",
    "        \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "        \"{% endif %}\"\n",
    "    )\n",
    "    print(\"‚úÖ Chat template —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "def format_for_training(example):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç messages –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è SFT.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º HuggingFace Dataset\n",
    "dataset = Dataset.from_list(sft_data)\n",
    "dataset = dataset.map(format_for_training, remove_columns=[\"metadata\"] if \"metadata\" in dataset.column_names else [])\n",
    "\n",
    "print(f\"\\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {dataset}\")\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä\n",
    "print(\"\\nüìù –ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:\")\n",
    "print(dataset[50][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –û–±—É—á–µ–Ω–∏–µ (SFT —Å QLoRA)\n",
    "\n",
    "### –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\n",
    "\n",
    "| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |\n",
    "|----------|----------|----------|\n",
    "| `per_device_train_batch_size` | 2 | –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ GPU |\n",
    "| `gradient_accumulation_steps` | 4 | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 2 * 4 = 8 |\n",
    "| `learning_rate` | 2e-4 | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π LR –¥–ª—è LoRA |\n",
    "| `num_train_epochs` | 3 | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö |\n",
    "| `warmup_ratio` | 0.1 | 10% —à–∞–≥–æ–≤ –Ω–∞ warmup |\n",
    "| `lr_scheduler_type` | cosine | –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ LR |\n",
    "| `fp16` | True | Mixed precision training |\n",
    "\n",
    "### –§–æ—Ä–º—É–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ batch size:\n",
    "```\n",
    "effective_batch = per_device_batch * gradient_accumulation * num_gpus\n",
    "                = 2 * 4 * 1 = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\ntraining_args = TrainingArguments(\n    output_dir=\"./shodim_almaty_qlora_output\",\n    \n    # Batch size\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,  # Effective batch = 8\n    \n    # –û–±—É—á–µ–Ω–∏–µ\n    num_train_epochs=3,             # 3 —ç–ø–æ—Ö–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n    learning_rate=2e-4,             # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π LR –¥–ª—è QLoRA\n    warmup_ratio=0.1,               # 10% warmup\n    lr_scheduler_type=\"cosine\",     # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ\n    \n    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n    optim=\"adamw_8bit\",             # 8-bit AdamW (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n    weight_decay=0.01,              # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n    max_grad_norm=1.0,              # Gradient clipping\n    \n    # Precision\n    fp16=True,                      # Mixed precision\n    bf16=False,                     # –ò—Å–ø–æ–ª—å–∑—É–µ–º FP16 (–±–æ–ª–µ–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ)\n    \n    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n    logging_steps=10,\n    logging_first_step=True,\n    \n    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n    save_strategy=\"epoch\",\n    save_total_limit=2,             # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ 2 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö checkpoint\n    \n    # –ü—Ä–æ—á–µ–µ\n    seed=42,\n    report_to=\"none\",               # –û—Ç–∫–ª—é—á–∞–µ–º W&B/TensorBoard\n)\n\n# –°–æ–∑–¥–∞—ë–º SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=True,                   # –£–ø–∞–∫–æ–≤–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ)\n    args=training_args,\n)\n\n# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\ntotal_steps = len(trainer.get_train_dataloader()) * training_args.num_train_epochs\nprint(f\"\\nüìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:\")\nprint(f\"   –≠–ø–æ—Ö: {training_args.num_train_epochs}\")\nprint(f\"   Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\nprint(f\"   –ü—Ä–∏–º–µ—Ä–Ω–æ —à–∞–≥–æ–≤: ~{total_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   Train Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Train Runtime: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "print(f\"   Samples/sec: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1 ORPO: Preference Optimization –¥–ª—è –¥—Ä—É–∂–µ–ª—é–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\n\n### –ß—Ç–æ —Ç–∞–∫–æ–µ ORPO?\n\n**ORPO (Odds Ratio Preference Optimization)** ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å –æ–¥–∏–Ω —Å—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–æ–≤ –¥—Ä—É–≥–æ–º—É:\n\n| –ê—Å–ø–µ–∫—Ç | SFT | ORPO |\n|--------|-----|------|\n| –î–∞–Ω–Ω—ã–µ | –¢–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã | –ü–∞—Ä—ã: chosen (—Ö–æ—Ä–æ—à–∏–π) vs rejected (–ø–ª–æ—Ö–æ–π) |\n| –¶–µ–ª—å | –ò–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã | –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å chosen –Ω–∞–¥ rejected |\n| –†–µ–∑—É–ª—å—Ç–∞—Ç | –ó–Ω–∞–µ—Ç —Ñ–∞–∫—Ç—ã | –ó–Ω–∞–µ—Ç —Ñ–∞–∫—Ç—ã + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–æ–Ω |\n\n### –ó–∞—á–µ–º –Ω—É–∂–µ–Ω ORPO –ø–æ—Å–ª–µ SFT?\n\n–ü–æ—Å–ª–µ SFT –º–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç **—á—Ç–æ** –æ—Ç–≤–µ—á–∞—Ç—å. ORPO —É—á–∏—Ç **–∫–∞–∫** –æ—Ç–≤–µ—á–∞—Ç—å:\n- **Chosen**: –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –∂–∏–≤–æ–π, —á–µ–ª–æ–≤–µ—á–Ω—ã–π —Å—Ç–∏–ª—å\n- **Rejected**: –°—É—Ö–æ–π, —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–∏–ª—å\n\n### –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö ORPO:\n```json\n{\n  \"prompt\": [{\"role\": \"user\", \"content\": \"–ö—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å?\"}],\n  \"chosen\": [{\"role\": \"assistant\", \"content\": \"–û, –æ—Ç–ª–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å! –û—á–µ–Ω—å —Å–æ–≤–µ—Ç—É—é...\"}],\n  \"rejected\": [{\"role\": \"assistant\", \"content\": \"–ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤–∞—Å –æ –Ω–∞–ª–∏—á–∏–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...\"}]\n}\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# –ó–∞–≥—Ä—É–∂–∞–µ–º ORPO –¥–∞—Ç–∞—Å–µ—Ç —Å preference –ø–∞—Ä–∞–º–∏\nORPO_DATASET_PATH = \"/content/orpo_dataset.json\"\n\nwith open(ORPO_DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n    orpo_data = json.load(f)\n\nprint(f\"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ preference –ø–∞—Ä: {len(orpo_data)}\")\n\n# –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä\nprint(\"\\nüìù –ü—Ä–∏–º–µ—Ä preference –ø–∞—Ä—ã:\")\nex = orpo_data[0]\nprint(f\"   [PROMPT]: {ex['prompt'][1]['content'][:80]}...\")\nprint(f\"   [CHOSEN]: {ex['chosen'][0]['content'][:80]}...\")\nprint(f\"   [REJECTED]: {ex['rejected'][0]['content'][:80]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è ORPO\nfrom trl import ORPOTrainer, ORPOConfig\n\ndef format_orpo_example(example):\n    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è ORPO —Ç—Ä–µ–Ω–µ—Ä–∞.\"\"\"\n    # Prompt (system + user)\n    prompt_text = tokenizer.apply_chat_template(\n        example[\"prompt\"],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    # Chosen response\n    chosen_text = tokenizer.apply_chat_template(\n        example[\"prompt\"] + example[\"chosen\"],\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    \n    # Rejected response  \n    rejected_text = tokenizer.apply_chat_template(\n        example[\"prompt\"] + example[\"rejected\"],\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    \n    return {\n        \"prompt\": prompt_text,\n        \"chosen\": chosen_text,\n        \"rejected\": rejected_text\n    }\n\n# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç\norpo_dataset = Dataset.from_list(orpo_data)\norpo_dataset = orpo_dataset.map(format_orpo_example, remove_columns=orpo_dataset.column_names)\n\nprint(f\"‚úÖ ORPO –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {orpo_dataset}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ORPO\norpo_config = ORPOConfig(\n    output_dir=\"./shodim_almaty_orpo_output\",\n    \n    # Batch size\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    \n    # –û–±—É—á–µ–Ω–∏–µ\n    num_train_epochs=2,              # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è preference tuning\n    learning_rate=5e-5,              # –ú–µ–Ω—å—à–∏–π LR –¥–ª—è fine-tuning\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    \n    # ORPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n    beta=0.1,                        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è odds ratio loss\n    max_length=1024,                 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞\n    max_prompt_length=512,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞\n    \n    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    \n    # Precision\n    fp16=True,\n    bf16=False,\n    \n    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n    logging_steps=10,\n    \n    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    \n    seed=42,\n    report_to=\"none\",\n)\n\n# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\nFastLanguageModel.for_training(model)\n\n# –°–æ–∑–¥–∞—ë–º ORPO Trainer\norpo_trainer = ORPOTrainer(\n    model=model,\n    args=orpo_config,\n    train_dataset=orpo_dataset,\n    tokenizer=tokenizer,\n)\n\nprint(f\"\\nüìä ORPO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\")\nprint(f\"   Beta (odds ratio): {orpo_config.beta}\")\nprint(f\"   Learning rate: {orpo_config.learning_rate}\")\nprint(f\"   –≠–ø–æ—Ö: {orpo_config.num_train_epochs}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# üöÄ –ó–∞–ø—É—Å–∫ ORPO –æ–±—É—á–µ–Ω–∏—è\nprint(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º ORPO –æ–±—É—á–µ–Ω–∏–µ (preference optimization)...\\n\")\n\norpo_result = orpo_trainer.train()\n\n# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\nprint(\"\\n\" + \"=\"*50)\nprint(\"‚úÖ ORPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\nprint(\"=\"*50)\nprint(f\"   Train Loss: {orpo_result.training_loss:.4f}\")\nprint(f\"   Train Runtime: {orpo_result.metrics['train_runtime']:.1f}s\")\n\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\nFINAL_OUTPUT_DIR = \"shodim_almaty_orpo_final\"\nmodel.save_pretrained(FINAL_OUTPUT_DIR)\ntokenizer.save_pretrained(FINAL_OUTPUT_DIR)\nprint(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {FINAL_OUTPUT_DIR}/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n\n–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ QLoRA –∞–¥–∞–ø—Ç–µ—Ä—ã. –û–Ω–∏ –∑–∞–Ω–∏–º–∞—é—Ç –≤—Å–µ–≥–æ ~100-200 MB."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°–æ—Ö—Ä–∞–Ω—è–µ–º QLoRA –∞–¥–∞–ø—Ç–µ—Ä—ã\nOUTPUT_DIR = \"shodim_almaty_qlora\"\n\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR}/\")\n\n# –ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä\nimport os\ntotal_size = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR))\nprint(f\"üì¶ –†–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: {total_size / 1e6:.1f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n\n### –ú–µ—Ç—Ä–∏–∫–∏:\n\n1. **BLEU** (Bilingual Evaluation Understudy)\n   - –ò–∑–º–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ n-–≥—Ä–∞–º–º –º–µ–∂–¥—É generated –∏ reference\n   - –î–∏–∞–ø–∞–∑–æ–Ω: 0-100, –≤—ã—à–µ = –ª—É—á—à–µ\n\n2. **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation)\n   - ROUGE-1: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —É–Ω–∏–≥—Ä–∞–º–º\n   - ROUGE-2: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –±–∏–≥—Ä–∞–º–º\n   - ROUGE-L: Longest Common Subsequence\n   - –î–∏–∞–ø–∞–∑–æ–Ω: 0-1, –≤—ã—à–µ = –ª—É—á—à–µ\n\n3. **BERTScore**\n   - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —á–µ—Ä–µ–∑ BERT embeddings\n   - –£—á–∏—Ç—ã–≤–∞–µ—Ç —Å–º—ã—Å–ª, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤\n   - –î–∏–∞–ø–∞–∑–æ–Ω: 0-1, –≤—ã—à–µ = –ª—É—á—à–µ\n\n### –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã:\n–í–æ–ø—Ä–æ—Å—ã —Å–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –≤ –ê–ª–º–∞—Ç—ã —Å sxodim.com."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –ê–ª–º–∞—Ç—ã\ntest_questions = [\n    # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n    \"–ö—É–¥–∞ –º–æ–∂–Ω–æ —Å—Ö–æ–¥–∏—Ç—å –≤ –ê–ª–º–∞—Ç—ã –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö?\",\n    \"–ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ –ê–ª–º–∞—Ç—ã —Å–µ–≥–æ–¥–Ω—è?\",\n    \"–ü–æ—Å–æ–≤–µ—Ç—É–π —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è –≤ –ê–ª–º–∞—Ç—ã\",\n    \n    # –°—Ç–µ–Ω–¥–∞–ø –∏ —é–º–æ—Ä\n    \"–ì–¥–µ –≤ –ê–ª–º–∞—Ç—ã –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–µ–Ω–¥–∞–ø?\",\n    \"–ö–∞–∫–∏–µ –∫–æ–º–µ–¥–∏–π–Ω—ã–µ —à–æ—É –µ—Å—Ç—å –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–°–∫–æ–ª—å–∫–æ —Å—Ç–æ—è—Ç –±–∏–ª–µ—Ç—ã –Ω–∞ —Å—Ç–µ–Ω–¥–∞–ø –≤ Punch?\",\n    \n    # –ö–æ–Ω—Ü–µ—Ä—Ç—ã\n    \"–ö–∞–∫–∏–µ –∫–æ–Ω—Ü–µ—Ä—Ç—ã –±—É–¥—É—Ç –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–ì–¥–µ –ø–æ—Å–ª—É—à–∞—Ç—å –¥–∂–∞–∑ –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–ß—Ç–æ —Ç–∞–∫–æ–µ EverJazz?\",\n    \n    # –ö–∏–Ω–æ\n    \"–ï—Å—Ç—å –ª–∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä—ã –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–∏–Ω–æ—É–∂–∏–Ω –æ—Ç VkusKino?\",\n    \n    # –ö–≤–∏–∑—ã –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è\n    \"–ì–¥–µ –ø–æ–∏–≥—Ä–∞—Ç—å –≤ –∫–≤–∏–∑ –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–ß—Ç–æ —Ç–∞–∫–æ–µ –ö–≤–∏–∑, –ø–ª–∏–∑?\",\n    \n    # –¢–µ–∞—Ç—Ä –∏ —Å–ø–µ–∫—Ç–∞–∫–ª–∏\n    \"–ö–∞–∫–∏–µ —Å–ø–µ–∫—Ç–∞–∫–ª–∏ –∏–¥—É—Ç –≤ –ê–ª–º–∞—Ç—ã?\",\n    \"–ö—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å —Å –¥–µ—Ç—å–º–∏ –≤ –ê–ª–º–∞—Ç—ã?\",\n]\n\n# –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\nreference_answers = [\n    \"–í –ê–ª–º–∞—Ç—ã –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –º–æ–∂–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç—å —Å—Ç–µ–Ω–¥–∞–ø-—à–æ—É –≤ Punch Stand Up Club, –¥–∂–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ—Ä—Ç—ã –≤ EverJazz, –∫–∏–Ω–æ–ø–æ–∫–∞–∑—ã –ø–æ–¥ –æ—Ç–∫—Ä—ã—Ç—ã–º –Ω–µ–±–æ–º –≤ Open Cinema –∏–ª–∏ –ø–æ–∏–≥—Ä–∞—Ç—å –≤ –ö–≤–∏–∑, –ø–ª–∏–∑!\",\n    \"–°–µ–≥–æ–¥–Ω—è –≤ –ê–ª–º–∞—Ç—ã –º–æ–∂–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç—å —Å—Ç–µ–Ω–¥–∞–ø-–∫–æ–Ω—Ü–µ—Ä—Ç—ã, –¥–∂–∞–∑–æ–≤—ã–µ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ –∫–ª—É–±–µ EverJazz, –∫–∏–Ω–æ–ø–æ–∫–∞–∑—ã –∏–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–≥—Ä—ã. –†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ñ–∏—à—É –Ω–∞ sxodim.com.\",\n    \"–í –ê–ª–º–∞—Ç—ã –º–Ω–æ–≥–æ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π: —Å—Ç–µ–Ω–¥–∞–ø –≤ Punch Stand Up Club (–æ—Ç 3000 —Ç–µ–Ω–≥–µ), –¥–∂–∞–∑ –≤ EverJazz, –∫–≤–∏–∑—ã –≤ –±–∞—Ä–∞—Ö, –∫–∏–Ω–æ—É–∂–∏–Ω—ã –æ—Ç VkusKino, —Å–ø–µ–∫—Ç–∞–∫–ª–∏ –∏ –∫–æ–Ω—Ü–µ—Ä—Ç—ã.\",\n    \"–°—Ç–µ–Ω–¥–∞–ø –≤ –ê–ª–º–∞—Ç—ã –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ Punch Stand Up Club –ø–æ –∞–¥—Ä–µ—Å—É —É–ª. –ö–∞–∏—Ä–±–µ–∫–æ–≤–∞, 35–ê. –¢–∞–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ—Ö–æ–¥—è—Ç —à–æ—É ¬´4–ö–æ–Ω—è¬ª, ¬´–°–∞–º–æ —Å–æ–±–æ–π¬ª –∏ –¥—Ä—É–≥–∏–µ –∫–æ–º–µ–¥–∏–π–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã.\",\n    \"–í –ê–ª–º–∞—Ç—ã –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–µ–¥–∏–π–Ω—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫: Punch Stand Up Club —Å —à–æ—É ¬´4–ö–æ–Ω—è¬ª –∏ –∏–º–ø—Ä–æ–≤–∏–∑–∞—Ü–∏—è–º–∏, Stand Up Camp —Å –∂—ë—Å—Ç–∫–∏–º —Å—Ç–µ–Ω–¥–∞–ø–æ–º –≤ —Ä–∞–∑–Ω—ã—Ö –±–∞—Ä–∞—Ö –≥–æ—Ä–æ–¥–∞.\",\n    \"–ë–∏–ª–µ—Ç—ã –Ω–∞ —Å—Ç–µ–Ω–¥–∞–ø –≤ Punch Stand Up Club —Å—Ç–æ—è—Ç –æ—Ç 2000-3000 —Ç–µ–Ω–≥–µ. –®–æ—É –ø—Ä–æ—Ö–æ–¥—è—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é, –Ω–∞—á–∞–ª–æ –æ–±—ã—á–Ω–æ –≤ 19:30 –∏–ª–∏ 21:30.\",\n    \"–í –ê–ª–º–∞—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –∫–æ–Ω—Ü–µ—Ä—Ç—ã —Ä–∞–∑–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤: –¥–∂–∞–∑ –≤ EverJazz, —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –º—É–∑—ã–∫–∞, —Ä–æ–∫-–∫–æ–Ω—Ü–µ—Ä—Ç—ã –≤ Motor Club, –∞ —Ç–∞–∫–∂–µ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö –∞—Ä—Ç–∏—Å—Ç–æ–≤.\",\n    \"–î–∂–∞–∑ –≤ –ê–ª–º–∞—Ç—ã –º–æ–∂–Ω–æ –ø–æ—Å–ª—É—à–∞—Ç—å –≤ –∫–ª—É–±–µ EverJazz –ø–æ –∞–¥—Ä–µ—Å—É —É–ª. –ì–æ–≥–æ–ª—è, 40–ë. –¢–∞–º –≤—ã—Å—Ç—É–ø–∞—é—Ç –º–µ—Å—Ç–Ω—ã–µ –¥–∂–∞–∑–º–µ–Ω—ã, –ø—Ä–æ—Ö–æ–¥—è—Ç jam session –ø–æ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞–º. –ë–∏–ª–µ—Ç—ã –æ—Ç 2000 —Ç–µ–Ω–≥–µ.\",\n    \"EverJazz ‚Äî —ç—Ç–æ –¥–∂–∞–∑-–∫–ª—É–± –≤ –ê–ª–º–∞—Ç—ã –Ω–∞ —É–ª. –ì–æ–≥–æ–ª—è, 40–ë. –ó–¥–µ—Å—å –ø—Ä–æ—Ö–æ–¥—è—Ç –∂–∏–≤—ã–µ –∫–æ–Ω—Ü–µ—Ä—Ç—ã, –¥–∂–µ–º-—Å–µ–π—à–Ω—ã –∏ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö –¥–∂–∞–∑–æ–≤—ã—Ö –º—É–∑—ã–∫–∞–Ω—Ç–æ–≤.\",\n    \"–î–∞, –≤ –ê–ª–º–∞—Ç—ã –µ—Å—Ç—å Open Cinema ‚Äî –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä –Ω–∞ –∫—Ä—ã—à–µ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–º –∫—É–ø–æ–ª–æ–º –Ω–∞ —É–ª. –ñ–∞–Ω–¥–æ—Å–æ–≤–∞ 58/1. –¢–∞–º –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ñ–∏–ª—å–º—ã —Å –≤–∏–¥–æ–º –Ω–∞ –≥–æ—Ä–æ–¥ –∏ –≥–æ—Ä—ã, –µ—Å—Ç—å –ø–ª–µ–¥—ã –∏ –±–∞—Ä.\",\n    \"–ö–∏–Ω–æ—É–∂–∏–Ω –æ—Ç VkusKino ‚Äî —ç—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –≥–¥–µ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ñ–∏–ª—å–º–∞ –ø–æ–¥–∞—é—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–ª—é–¥–∞ –∏ –∫–æ–∫—Ç–µ–π–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ ¬´–®—Ä–µ–∫ 2¬ª –ø–æ–¥–∞—é—Ç –≤–æ–ª—à–µ–±–Ω—ã–µ –∑–µ–ª—å—è –∏ –±–ª—é–¥–∞ –∏–∑ —Å–∫–∞–∑–∫–∏.\",\n    \"–ö–≤–∏–∑—ã –≤ –ê–ª–º–∞—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –≤ —Ä–∞–∑–Ω—ã—Ö –±–∞—Ä–∞—Ö. –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π ‚Äî –ö–≤–∏–∑, –ø–ª–∏–∑! –≤ Guinness pub –Ω–∞ —É–ª. –î–æ—Å—Ç—ã–∫, 71. –ë–∏–ª–µ—Ç—ã –æ—Ç 3000 —Ç–µ–Ω–≥–µ —Å —á–µ–ª–æ–≤–µ–∫–∞.\",\n    \"–ö–≤–∏–∑, –ø–ª–∏–∑! ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞, –≥–¥–µ –∫–æ–º–∞–Ω–¥—ã –∏–∑ 2-9 —á–µ–ª–æ–≤–µ–∫ –æ—Ç–≤–µ—á–∞—é—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. 7 —Ä–∞—É–Ω–¥–æ–≤, 39 –≤–æ–ø—Ä–æ—Å–æ–≤, –æ–∫–æ–ª–æ 2 —á–∞—Å–æ–≤. –ò–≥—Ä–∞—é—Ç –±–æ–ª–µ–µ 200 —Ç—ã—Å—è—á –∫–æ–º–∞–Ω–¥ –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É.\",\n    \"–í –ê–ª–º–∞—Ç—ã –∏–¥—É—Ç —Ä–∞–∑–Ω—ã–µ —Å–ø–µ–∫—Ç–∞–∫–ª–∏: ¬´–°–∏—Ä–æ—Ç–ª–∏–≤—ã–π –∑–∞–ø–∞–¥¬ª, ¬´–ú–µ—á—Ç–∞¬ª, ¬´–ò—Å—Ç–æ—Ä–∏—è –¥–≤—É—Ö —Ä—ã—Ü–∞—Ä–µ–π¬ª, –º—É–∑—ã–∫–∞–ª—å–Ω—ã–π —Å–ø–µ–∫—Ç–∞–∫–ª—å ¬´–°–æ—Å—Ç–æ—è–Ω–∏–µ –¢–∞–Ω–≥–æ¬ª –∏ –¥—Ä—É–≥–∏–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\",\n    \"–° –¥–µ—Ç—å–º–∏ –≤ –ê–ª–º–∞—Ç—ã –º–æ–∂–Ω–æ —Å—Ö–æ–¥–∏—Ç—å –Ω–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ü–∏—Ä–∫ –®–∞–ø–∏—Ç–æ, –¥–µ—Ç—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ Oi-Qaragai (–ª—ã–∂–∏, —é—Ä—Ç—ã), –º—É–ª—å—Ç—Ñ–∏–ª—å–º—ã –≤ Open Cinema –∏–ª–∏ –∫–∏–Ω–æ—É–∂–∏–Ω—ã —Å —Å–µ–º–µ–π–Ω—ã–º–∏ —Ñ–∏–ª—å–º–∞–º–∏.\",\n]\n\nprint(f\"üìã –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {len(test_questions)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\nFastLanguageModel.for_inference(model)\n\n# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç\nSYSTEM_PROMPT = \"–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –≤ –ê–ª–º–∞—Ç—ã. –ü–æ–º–æ–≥–∞–µ—à—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω–∞–π—Ç–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è, –∫–æ–Ω—Ü–µ—Ä—Ç—ã, —Å–ø–µ–∫—Ç–∞–∫–ª–∏, –≤—ã—Å—Ç–∞–≤–∫–∏ –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è –≤ –≥–æ—Ä–æ–¥–µ. –û—Ç–≤–µ—á–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ, —É–∫–∞–∑—ã–≤–∞—è –¥–∞—Ç—ã, –≤—Ä–µ–º—è, –º–µ—Å—Ç–æ –∏ —Ü–µ–Ω—ã.\"\n\ndef generate_answer(question: str, max_tokens: int = 200) -> str:\n    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": question}\n    ]\n    \n    # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n    \n    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n    if \"assistant\" in full_response.lower():\n        parts = full_response.split(\"assistant\")\n        answer = parts[-1].strip()\n    else:\n        answer = full_response\n    \n    return answer\n\nprint(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤–∞\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤—Å–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "print(\"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤...\\n\")\n",
    "\n",
    "generated_answers = []\n",
    "\n",
    "for i, question in enumerate(tqdm(test_questions, desc=\"Generating\")):\n",
    "    answer = generate_answer(question)\n",
    "    generated_answers.append(answer)\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    if i < 5:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚ùì Q{i+1}: {question}\")\n",
    "        print(f\"ü§ñ Generated: {answer[:200]}...\" if len(answer) > 200 else f\"ü§ñ Generated: {answer}\")\n",
    "        print(f\"üìö Reference: {reference_answers[i][:200]}...\" if len(reference_answers[i]) > 200 else f\"üìö Reference: {reference_answers[i]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –æ—Ç–≤–µ—Ç–æ–≤: {len(generated_answers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "print(\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BLEU Score\n",
    "print(\"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BLEU...\")\n",
    "references_bleu = [[ref] for ref in reference_answers]  # BLEU —Ç—Ä–µ–±—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤\n",
    "\n",
    "bleu_result = bleu_metric.compute(\n",
    "    predictions=generated_answers,\n",
    "    references=references_bleu\n",
    ")\n",
    "\n",
    "print(f\"   BLEU Score: {bleu_result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ROUGE Scores\n",
    "print(\"\\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ ROUGE...\")\n",
    "\n",
    "rouge_result = rouge_metric.compute(\n",
    "    predictions=generated_answers,\n",
    "    references=reference_answers\n",
    ")\n",
    "\n",
    "print(f\"   ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "print(f\"   ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
    "print(f\"   ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "print(f\"   ROUGE-Lsum: {rouge_result['rougeLsum']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BERTScore (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)\n",
    "print(\"\\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BERTScore (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...\")\n",
    "\n",
    "bertscore_result = bertscore_metric.compute(\n",
    "    predictions=generated_answers,\n",
    "    references=reference_answers,\n",
    "    lang=\"ru\",  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫\n",
    "    model_type=\"bert-base-multilingual-cased\"  # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    ")\n",
    "\n",
    "# –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "precision = sum(bertscore_result['precision']) / len(bertscore_result['precision'])\n",
    "recall = sum(bertscore_result['recall']) / len(bertscore_result['recall'])\n",
    "f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
    "\n",
    "print(f\"   BERTScore Precision: {precision:.4f}\")\n",
    "print(f\"   BERTScore Recall: {recall:.4f}\")\n",
    "print(f\"   BERTScore F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã:\n\n1. ‚úÖ –ó–∞–≥—Ä—É–∑–∏–ª–∏ Qwen2.5-3B —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (QLoRA)\n2. ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (r=32, alpha=64)\n3. ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ SFT –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1443 Q&A –ø–∞—Ä –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –ê–ª–º–∞—Ç—ã\n4. ‚úÖ –û–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å —Å SFTTrainer (–∑–Ω–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤)\n5. ‚úÖ –ü—Ä–∏–º–µ–Ω–∏–ª–∏ ORPO –¥–ª—è –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–≥–æ —Å—Ç–∏–ª—è –æ—Ç–≤–µ—Ç–æ–≤\n6. ‚úÖ –û—Ü–µ–Ω–∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é BLEU, ROUGE –∏ BERTScore\n\n### –ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è:\n```\n–ë–∞–∑–∞ (Qwen2.5-3B) ‚Üí SFT (—Ñ–∞–∫—Ç—ã –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö) ‚Üí ORPO (–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Å—Ç–∏–ª—å)\n```\n\n### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–æ–≤—ã–º–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º–∏\n- –î–æ–±–∞–≤–∏—Ç—å RAG –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ–±—ã—Ç–∏—è—Ö\n- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å Telegram-–±–æ—Ç–æ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n- –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ HuggingFace Hub –¥–ª—è sharing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n\n–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö –≤ –ê–ª–º–∞—Ç—ã!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ç–µ—Å—Ç\ndef chat(question: str):\n    \"\"\"–ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.\"\"\"\n    print(f\"\\n‚ùì –í–æ–ø—Ä–æ—Å: {question}\")\n    answer = generate_answer(question, max_tokens=300)\n    print(f\"\\nü§ñ –û—Ç–≤–µ—Ç: {answer}\")\n    print(\"-\" * 60)\n\n# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\nchat(\"–ö—É–¥–∞ —Å—Ö–æ–¥–∏—Ç—å –≤ –ê–ª–º–∞—Ç—ã –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö?\")\nchat(\"–ì–¥–µ –ø–æ—Å–ª—É—à–∞—Ç—å –∂–∏–≤—É—é –º—É–∑—ã–∫—É –≤ –ê–ª–º–∞—Ç—ã?\")\nchat(\"–ö–∞–∫–∏–µ —Å—Ç–µ–Ω–¥–∞–ø-—à–æ—É –µ—Å—Ç—å –≤ Punch Stand Up Club?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n\n–ú–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –∏–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ GGUF –¥–ª—è llama.cpp."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –û–ø—Ü–∏—è 1: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å merged –º–æ–¥–µ–ª—å –≤ 16-bit (–¥–ª—è HuggingFace Hub)\n# model.save_pretrained_merged(\"shodim_almaty_merged_16bit\", tokenizer, save_method=\"merged_16bit\")\n\n# –û–ø—Ü–∏—è 2: –≠–∫—Å–ø–æ—Ä—Ç –≤ GGUF –¥–ª—è llama.cpp (–ª–æ–∫–∞–ª—å–Ω—ã–π inference)\n# model.save_pretrained_gguf(\"shodim_almaty_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n\nprint(\"üí° –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –Ω—É–∂–Ω—É—é –æ–ø—Ü–∏—é –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã:\n\n1. ‚úÖ –ó–∞–≥—Ä—É–∑–∏–ª–∏ Qwen2.5-3B —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (QLoRA)\n2. ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (r=32, alpha=64)\n3. ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1443 Q&A –ø–∞—Ä –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º –ê–ª–º–∞—Ç—ã (sxodim.com)\n4. ‚úÖ –û–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å —Å SFTTrainer\n5. ‚úÖ –û—Ü–µ–Ω–∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é BLEU, ROUGE –∏ BERTScore\n\n### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–æ–≤—ã–º–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º–∏\n- –î–æ–±–∞–≤–∏—Ç—å RAG –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ–±—ã—Ç–∏—è—Ö\n- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å Telegram-–±–æ—Ç–æ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n- –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ HuggingFace Hub –¥–ª—è sharing"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}